# -*- coding: utf-8 -*-
"""Gender_Classification_Conventional_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q7wCxxnuEG7EB-cqJYlHpJlw6HLqIbag
"""

from google.colab import drive
drive.mount('/content/drive')

# Python Pckages
import random
import pandas as pd
import numpy as np
from tabulate import tabulate

# ML Packages
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# ML Classifiers
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

# ML Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def data_preprocessing(df):
  # Removing NaN Data
  df = df.dropna()

  # Seperating Male Indices and Female Indices
  male_df = []
  female_df = []
  for i in range(df.shape[0]):
    if df.iloc[i, 1] == 'male':
      male_df.append(i)
    elif df.iloc[i, 1] == 'female':
      female_df.append(i)
    df.iloc[i, 0] = str(df.iloc[i, 0]).lower()
    
  # Creating New Dataset where Number of Male == Number of Female
  sampled_indices = list(random.sample(male_df, len(female_df))) + female_df
  sampled_df = df.iloc[sampled_indices, :]

  # Converting String Class to Numeric Value
  encoder = preprocessing.LabelEncoder()
  sampled_df.iloc[:, 1] = encoder.fit_transform(sampled_df.iloc[:, 1])
  
  sampled_df = sampled_df.sample(frac=1)

  return sampled_df

def vectorization(X_train, X_test, vectorizer, analyzer, max_features):
  if vectorizer == "CV":
    count_vectorizer = CountVectorizer(analyzer=analyzer, max_features=max_features)
    X_train_transformed = count_vectorizer.fit_transform(X_train)
    if X_test is not None:
      X_test_transformed = count_vectorizer.transform(X_test)
      return X_train_transformed, X_test_transformed
    else:
      return X_train_transformed, count_vectorizer
  elif vectorizer == "TFIDF":
    tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer, max_features=max_features)
    X_train_transformed = tfidf_vectorizer.fit_transform(X_train)
    if X_test is not None:
      X_test_transformed = tfidf_vectorizer.transform(X_test)
      return X_train_transformed, X_test_transformed  
    else:
      return X_train_transformed, tfidf_vectorizer

# Load our data
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Filtered_Dataset.csv")
df.head()

processed_df = data_preprocessing(df)
processed_df.head(10)

X = np.array(processed_df.iloc[:, 0])
y = np.array(processed_df.iloc[:, 1])

# classifiers = [LogisticRegression(), MultinomialNB(), RandomForestClassifier(), 
#                SVC(), AdaBoostClassifier(), GradientBoostingClassifier()]
# classifiers

# classifier_names = ['Logistic Regression', 'Multinomial NB', 'Random Forest', 'SVC', 'AdaBoost', 'GradientBoost']

# skf = StratifiedKFold(n_splits=5)
# CV_train_table   = []
# CV_test_table = []
# for i in range(len(classifier_names)):
#   print(f"\n### Classifier - > {classifier_names[i]}\n")

#   train_accuracies = []
#   train_precisions = []
#   train_recalls    = []
#   train_f1_scores  = []

#   test_accuracies  = []
#   test_precisions  = []
#   test_recalls     = []
#   test_f1_scores   = []

#   CV = 0
#   for train_index, test_index in skf.split(X, y):
#     CV += 1
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]

#     X_train_transformed, X_test_transformed = vectorization(X_train, X_test, vectorizer="CV", 
#                                                          analyzer="word", max_features=2500)
    
#     classifiers[i].fit(X_train_transformed, y_train)
#     train_prediction = classifiers[i].predict(X_train_transformed)
#     test_prediction  = classifiers[i].predict(X_test_transformed)

#     train_accuracy  = accuracy_score(y_train, train_prediction)
#     train_precision = precision_score(y_train, train_prediction, average='weighted')
#     train_recall    = recall_score(y_train, train_prediction, average='weighted')
#     train_f1_score  = f1_score(y_train, train_prediction, average='weighted')

#     test_accuracy  = accuracy_score(y_test, test_prediction)
#     test_precision = precision_score(y_test, test_prediction, average='weighted')
#     test_recall    = recall_score(y_test, test_prediction, average='weighted')
#     test_f1_score  = f1_score(y_test, test_prediction, average='weighted')

#     train_accuracies.append(train_accuracy)
#     train_precisions.append(train_precision)
#     train_recalls.append(train_recall)
#     train_f1_scores.append(train_f1_score)

#     test_accuracies.append(test_accuracy)
#     test_precisions.append(test_precision)
#     test_recalls.append(test_recall)
#     test_f1_scores.append(test_f1_score)

#     print(f"CV {CV}")


#   CV_train_table.append([classifier_names[i],
#                    round(np.mean(train_accuracies) * 100, 2),
#                    round(np.mean(train_precisions) * 100, 2),
#                    round(np.mean(train_recalls) * 100, 2),
#                    round(np.mean(train_f1_scores) * 100, 2)])
#   CV_test_table.append([classifier_names[i],
#                    round(np.mean(test_accuracies) * 100, 2),
#                    round(np.mean(test_precisions) * 100, 2),
#                    round(np.mean(test_recalls) * 100, 2),
#                    round(np.mean(test_f1_scores) * 100, 2)])  

# print("\n\n## CV Train Result ##\n")
# print(tabulate(CV_train_table, headers= ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-Score']))

# print("\n\n## CV Test Result ##\n")
# print(tabulate(CV_test_table, headers= ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-Score']))

X_transformed, vectorzier = vectorization(X, None, vectorizer="TFIDF", 
                           analyzer="word", max_features=2500)

classifier = MultinomialNB()
classifier.fit(X_transformed, y)

test_names = ["Rayhan Rashid","Rani",
              "Afsana Yeasmin Mili",
              "Rayhana",
              "Md. Zahid Hossain Khan", 
              "Shahriar Haque", 
              "Ashique Jubayer",
              "Nowsin", 
              "Shajedul Islam",
              "Saif Ahmed Anik", 
              "Toriqul Islam",
              "Afrina Yeasmin Mithila",
              "MD. Khalid Hossain Khan",
              "Md. Faruque Hossain Khan",
              "Md. Abdul Aziz Khan",
              "Samsun Nahar",
              "Farhana Sharmin",
              "Mahbub Azam",
              "Roni Hauladar", 
              "Chowdhuri Mofizur Rahman",
              "Sajid Ahmed",
              "Swakkhar Swatabda",
              "Sabila Nowshin",
              "Rifat Jabin",
              "Tithi",
              "Tanvir Rashid",
              "Kamrul Islam Tushar",
              "Sahanaj",
              "Suma Akhter",
              "Nishat Fariha",
              "Faiza Alam",
              "Ripon Khan",
              "Ridita Afrin"]
test_vectors = vectorzier.transform(test_names).toarray()

test_predictions = list(classifier.predict(test_vectors))
test_predictions = ['M' if prediction == 1 else 'F' for prediction in test_predictions]

test_table = []
for i in range(len(test_names)):
  test_table.append([test_names[i], test_predictions[i]])
print("\n\n## Test Names Result ##\n")
print(tabulate(test_table, headers= ['Name', 'Gender']))

# Load test data
test_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Test_Dataset.csv")
test_df.head()

print(test_df.shape)
test_df = test_df.dropna()
print(test_df.shape)

for i in range(test_df.shape[0]):
  test_df.iloc[i, 0], test_df.iloc[i, 1] = str(test_df.iloc[i, 0]).lower(), str(test_df.iloc[i, 1]).lower()
  if test_df.iloc[i, 1] == 'male':
    test_df.iloc[i, 1] = 'M'
  else:
    test_df.iloc[i, 1] = 'F'
test_df.head()

test_X, test_y = list(test_df.iloc[:, 0]), list(test_df.iloc[:, 1])
test_vectors = vectorzier.transform(test_X).toarray()

test_predictions = list(classifier.predict(test_vectors))
test_predictions = ['M' if prediction == 1 else 'F' for prediction in test_predictions]

accuracy = accuracy_score(test_y, test_predictions)
print(f"Accuracy : {round(accuracy * 100, 2)}%")

result = pd.DataFrame(list(zip(test_X, test_y, test_predictions)), columns=['Name', 'Original Gender', 'Predicted Gender'])
result.to_csv('/content/drive/MyDrive/Colab Notebooks/Result(Conventional).csv', index=False)

# ##### Count Vectorizer -> Analyzer = Word, Max Features = 5000 #####

# ## CV Train Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       99.08        99.09     99.08       99.08
# Multinomial NB            99.22        99.22     99.22       99.22
# Random Forest             99.99        99.99     99.99       99.99
# SVC                       99.11        99.11     99.11       99.11
# AdaBoost                  89           90.43     89          88.91
# GradientBoost             89.3         91.02     89.3        89.19


# ## CV Test Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       93.88        94.02     93.88       93.87
# Multinomial NB            95.24        95.29     95.24       95.23
# Random Forest             93.06        93.26     93.06       93.06
# SVC                       92.67        92.94     92.67       92.66
# AdaBoost                  88.65        90.12     88.65       88.54
# GradientBoost             88.6         90.41     88.6        88.47

# ##### Count Vectorizer -> Analyzer = Char, Max Features = 5000 #####

# ## CV Train Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       86.88        86.94     86.88       86.87
# Multinomial NB            84.95        84.97     84.95       84.95
# Random Forest             99.97        99.97     99.97       99.97
# SVC                       92.46        92.51     92.46       92.46
# AdaBoost                  87.09        87.12     87.09       87.09
# GradientBoost             90.05        90.06     90.05       90.05


# ## CV Test Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       86.84        86.89     86.84       86.83
# Multinomial NB            84.89        84.92     84.89       84.89
# Random Forest             90.59        90.61     90.59       90.59
# SVC                       90.26        90.32     90.26       90.26
# AdaBoost                  86.73        86.77     86.73       86.73
# GradientBoost             88.6         88.61     88.6        88.6

# ##### TFIDF Vectorizer -> Analyzer = Word, Max Features = 5000 #####

# ## CV Train Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       98.55        98.56     98.55       98.55
# Multinomial NB            99.41        99.42     99.41       99.41
# Random Forest             99.97        99.97     99.97       99.97
# SVC                       99.89        99.89     99.89       99.89
# AdaBoost                  88.81        90.51     88.81       88.69
# GradientBoost             89.74        91.17     89.74       89.65


# ## CV Test Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       94.19        94.41     94.19       94.19
# Multinomial NB            95.45        95.51     95.45       95.44
# Random Forest             93.12        93.39     93.12       93.11
# SVC                       94.21        94.42     94.21       94.2
# AdaBoost                  88.65        90.27     88.65       88.53
# GradientBoost             89.28        90.71     89.28       89.18

# ##### TFIDF Vectorizer -> Analyzer = Char, Max Features = 5000 #####

# ## CV Train Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       86.87        86.88     86.87       86.87
# Multinomial NB            83.94        83.95     83.94       83.94
# Random Forest             99.99        99.99     99.99       99.99
# SVC                       93.55        93.56     93.55       93.55
# AdaBoost                  87.15        87.16     87.15       87.15
# GradientBoost             91.09        91.09     91.09       91.09


# ## CV Test Result ##

# Classifier             Accuracy    Precision    Recall    F1-Score
# -------------------  ----------  -----------  --------  ----------
# Logistic Regression       86.46        86.49     86.46       86.46
# Multinomial NB            83.75        83.76     83.75       83.74
# Random Forest             89.52        89.55     89.52       89.52
# SVC                       90.05        90.07     90.05       90.05
# AdaBoost                  86.01        86.03     86.01       86.01
# GradientBoost             87.94        87.96     87.94       87.94